{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6dd7969-bf33-4ee5-8a21-2cb1334babba",
      "metadata": {
        "id": "f6dd7969-bf33-4ee5-8a21-2cb1334babba"
      },
      "source": [
        "This code invokes an LLM (gpt-4o-mini) with the provided prompt and returns the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64b00562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c531ca9-ff3a-422e-d153-11bf1b49a44c"
      },
      "source": [
        "pip install python-frontmatter\n"
      ],
      "id": "64b00562",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-frontmatter\n",
            "  Downloading python_frontmatter-1.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from python-frontmatter) (6.0.3)\n",
            "Downloading python_frontmatter-1.1.0-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: python-frontmatter\n",
            "Successfully installed python-frontmatter-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cf58f6ed-aaa5-4dc8-ad06-b5328a3b78b9",
      "metadata": {
        "id": "cf58f6ed-aaa5-4dc8-ad06-b5328a3b78b9"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import frontmatter\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e839caa8-7fd6-4f6e-9c57-64fbd6d5b73f",
      "metadata": {
        "id": "e839caa8-7fd6-4f6e-9c57-64fbd6d5b73f"
      },
      "outputs": [],
      "source": [
        "def read_repo_data(repo_owner, repo_name):\n",
        "    \"\"\"\n",
        "    Download and parse all markdown files from a GitHub repository.\n",
        "\n",
        "    Args:\n",
        "        repo_owner: GitHub username or organization\n",
        "        repo_name: Repository name\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing file content and metadata\n",
        "    \"\"\"\n",
        "    prefix = 'https://codeload.github.com'\n",
        "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
        "    resp = requests.get(url)\n",
        "\n",
        "    if resp.status_code != 200:\n",
        "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
        "\n",
        "    repository_data = []\n",
        "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
        "\n",
        "    for file_info in zf.infolist():\n",
        "        filename = file_info.filename\n",
        "        filename_lower = filename.lower()\n",
        "\n",
        "        if not (filename_lower.endswith('.md')\n",
        "            or filename_lower.endswith('.mdx')):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with zf.open(file_info) as f_in:\n",
        "                content = f_in.read().decode('utf-8', errors='ignore')\n",
        "                post = frontmatter.loads(content)\n",
        "                data = post.to_dict()\n",
        "                data['filename'] = filename\n",
        "                repository_data.append(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    zf.close()\n",
        "    return repository_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ff74cd18-4f51-4a23-abbb-790ce0fa86cf",
      "metadata": {
        "id": "ff74cd18-4f51-4a23-abbb-790ce0fa86cf"
      },
      "outputs": [],
      "source": [
        "def sliding_window(seq, size, step):\n",
        "    if size <= 0 or step <= 0:\n",
        "        raise ValueError(\"size and step must be positive\")\n",
        "\n",
        "    n = len(seq)\n",
        "    result = []\n",
        "    for i in range(0, n, step):\n",
        "        chunk = seq[i:i+size]\n",
        "        result.append({'start': i, 'chunk': chunk})\n",
        "        if i + size >= n:\n",
        "            break\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f6574048-10d4-4d19-b298-a64082cd3371",
      "metadata": {
        "id": "f6574048-10d4-4d19-b298-a64082cd3371"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def split_markdown_by_level(text, level=2):\n",
        "    \"\"\"\n",
        "    Split markdown text by a specific header level.\n",
        "\n",
        "    :param text: Markdown text as a string\n",
        "    :param level: Header level to split on\n",
        "    :return: List of sections as strings\n",
        "    \"\"\"\n",
        "    # This regex matches markdown headers\n",
        "    # For level 2, it matches lines starting with \"## \"\n",
        "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
        "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
        "\n",
        "    # Split and keep the headers\n",
        "    parts = pattern.split(text)\n",
        "\n",
        "    sections = []\n",
        "    for i in range(1, len(parts), 3):\n",
        "        # We step by 3 because regex.split() with\n",
        "        # capturing groups returns:\n",
        "        # [before_match, group1, group2, after_match, ...]\n",
        "        # here group1 is \"## \", group2 is the header text\n",
        "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
        "        header = header.strip()\n",
        "\n",
        "        # Get the content after this header\n",
        "        content = \"\"\n",
        "        if i+2 < len(parts):\n",
        "            content = parts[i+2].strip()\n",
        "\n",
        "        if content:\n",
        "            section = f'{header}\\n\\n{content}'\n",
        "        else:\n",
        "            section = header\n",
        "        sections.append(section)\n",
        "\n",
        "    return sections\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cf60b328-eaee-4d4a-9f0e-20ed6a3f6fb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf60b328-eaee-4d4a-9f0e-20ed6a3f6fb8",
        "outputId": "5eb6ee1e-80b1-48d2-9b30-243faa0356f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<minsearch.minsearch.Index at 0x7f91263426f0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from minsearch import Index\n",
        "fin_faq = [\n",
        "    {\"question\": \"What was Apple's revenue in Q4 2022?\", \"answer\": \"Apple reported revenue of $90.15 billion in Q4 2022.\"},\n",
        "    {\"question\": \"Does Tesla disclose revenue from regulatory credits?\", \"answer\": \"Yes, Tesla reported $286 million in revenue from regulatory credits in Q2 2022.\"},\n",
        "    {\"question\": \"What percentage of Microsoft’s revenue comes from cloud services?\", \"answer\": \"In FY2022, Microsoft reported that 51% of its revenue came from cloud-based services.\"},\n",
        "    {\"question\": \"How much cash did Alphabet hold at the end of 2021?\", \"answer\": \"Alphabet reported cash and cash equivalents of $20.9 billion at the end of 2021.\"},\n",
        "    {\"question\": \"Did Amazon’s advertising revenue grow in 2021?\", \"answer\": \"Yes, Amazon’s advertising revenue grew 32% in 2021, reaching $31 billion.\"}\n",
        "]\n",
        "fin_index = Index(\n",
        "    text_fields=[\"question\", \"content\"],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "\n",
        "fin_index.fit(fin_faq)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fin_index = Index(\n",
        "    text_fields=[\"question\", \"answer\"],\n",
        "    keyword_fields=[]\n",
        ")\n",
        "\n",
        "fin_index.fit(fin_faq)"
      ],
      "metadata": {
        "id": "cmuQcMTF9bZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0f348d-cb6a-41e6-cd1c-d15aec012d27"
      },
      "id": "cmuQcMTF9bZa",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<minsearch.minsearch.Index at 0x7f9126342660>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae474f64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b418f1-9398-4a0c-c8b6-60b5634952e4"
      },
      "source": [
        "%pip install minsearch"
      ],
      "id": "ae474f64",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minsearch\n",
            "  Downloading minsearch-0.0.7-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from minsearch) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from minsearch) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from minsearch) (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->minsearch) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->minsearch) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->minsearch) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->minsearch) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->minsearch) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->minsearch) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->minsearch) (1.17.0)\n",
            "Downloading minsearch-0.0.7-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: minsearch\n",
            "Successfully installed minsearch-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "548dce64-f79f-4fc5-9a5e-40c42718e31b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335,
          "referenced_widgets": [
            "94f31ef13c224820951c1ea56985b42d",
            "5dfb81d901f041d0aec4715ad46abad1",
            "b5be2cb5660145de98e45e5c3288c07b",
            "9df1e2264aae49de86ff665e28c8a5a8",
            "78a73ecb7fe54a7e8d37647e2a5ca5f4",
            "35f7fc392e4f44d7b43ed1cbb52e9665",
            "8374e65734044cd289d0a463fc7d1b82",
            "7460c485045146dab83a450caf7ccd55",
            "9d749ad8d30d4112b70684a0dee8c273",
            "1e45c83e78694e9782006150d8639df6",
            "5d05715b10654fc087cf3131d44792b1"
          ]
        },
        "id": "548dce64-f79f-4fc5-9a5e-40c42718e31b",
        "outputId": "fe518362-0bc2-4864-865c-b42fe03942ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94f31ef13c224820951c1ea56985b42d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-2.13579237e-02  2.66592316e-02  1.79311540e-02 ...  3.21754962e-02\n",
            "   4.94720824e-02 -1.65575575e-02]\n",
            " [ 4.85714786e-02  2.07287185e-02  3.62497456e-02 ... -2.79007219e-02\n",
            "   3.08107361e-02 -1.47657096e-03]\n",
            " [-3.70786823e-02 -2.26216391e-02  9.21595097e-03 ...  4.05022828e-03\n",
            "   8.13180655e-02  3.19980308e-02]\n",
            " [-3.88025306e-02 -6.83000870e-03 -6.74111443e-03 ...  2.57157721e-02\n",
            "   7.62579441e-02  2.03968734e-02]\n",
            " [ 1.74079859e-03  1.16550555e-05  2.63940655e-02 ... -5.47024515e-03\n",
            "   1.11732632e-01  3.12457941e-02]]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
        "\n",
        "fin_embeddings = []\n",
        "\n",
        "for d in tqdm(fin_faq):\n",
        "    # Check if 'question' and 'answer' keys exist before accessing them\n",
        "    if 'question' in d and 'answer' in d:\n",
        "        text = d['question'] + ' ' + d['answer']\n",
        "        v = embedding_model.encode(text)\n",
        "        fin_embeddings.append(v)\n",
        "    else:\n",
        "        print(f\"Skipping document due to missing 'question' or 'answer' key: {d.get('filename', 'Unknown file')}\")\n",
        "\n",
        "\n",
        "fin_embeddings = np.array(fin_embeddings)\n",
        "print(fin_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac7d8dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "293d203b-5202-40c2-a138-0a4365fd35f8"
      },
      "source": [
        "display(fin_faq)"
      ],
      "id": "ac7d8dc8",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[{'question': \"What was Apple's revenue in Q4 2022?\",\n",
              "  'answer': 'Apple reported revenue of $90.15 billion in Q4 2022.'},\n",
              " {'question': 'Does Tesla disclose revenue from regulatory credits?',\n",
              "  'answer': 'Yes, Tesla reported $286 million in revenue from regulatory credits in Q2 2022.'},\n",
              " {'question': 'What percentage of Microsoft’s revenue comes from cloud services?',\n",
              "  'answer': 'In FY2022, Microsoft reported that 51% of its revenue came from cloud-based services.'},\n",
              " {'question': 'How much cash did Alphabet hold at the end of 2021?',\n",
              "  'answer': 'Alphabet reported cash and cash equivalents of $20.9 billion at the end of 2021.'},\n",
              " {'question': 'Did Amazon’s advertising revenue grow in 2021?',\n",
              "  'answer': 'Yes, Amazon’s advertising revenue grew 32% in 2021, reaching $31 billion.'}]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pydantic AI(Agentic Library"
      ],
      "metadata": {
        "id": "11wnjLfXoVq1"
      },
      "id": "11wnjLfXoVq1"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Any\n",
        "\n",
        "def text_search(query: str) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Perform a text-based search on the FAQ index.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query string.\n",
        "\n",
        "    Returns:\n",
        "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
        "    \"\"\"\n",
        "    return fin_index.search(query, num_results=2)\n"
      ],
      "metadata": {
        "id": "loddS30CoUI7"
      },
      "id": "loddS30CoUI7",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a  course.\n",
        "\n",
        "Use the search tool to find relevant information from the course materials before answering questions.\n",
        "\n",
        "If you can find specific information through search, use it to provide accurate answers.\n",
        "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gs-qXwjmadK5"
      },
      "id": "gs-qXwjmadK5",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "agent = Agent(\n",
        "     name=\"faq_agent\",\n",
        "     instructions=system_prompt,\n",
        "     tools=[text_search],\n",
        "     model='gpt-4o-mini'\n",
        " )"
      ],
      "metadata": {
        "id": "NBZXbnh7pKiu"
      },
      "id": "NBZXbnh7pKiu",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What was Apple's revenue in Q4 2022\"\n",
        "\n"
      ],
      "metadata": {
        "id": "33VyQmLPDOmO"
      },
      "id": "33VyQmLPDOmO",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n"
      ],
      "metadata": {
        "id": "-0ajHgBzDVNH"
      },
      "id": "-0ajHgBzDVNH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajQWBTJ65bcb"
      },
      "id": "ajQWBTJ65bcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = await agent.run(user_prompt=question)"
      ],
      "metadata": {
        "id": "u7miq-nl5Zsa"
      },
      "id": "u7miq-nl5Zsa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.new_messages()\n"
      ],
      "metadata": {
        "id": "s_QXr2y7anvh"
      },
      "id": "s_QXr2y7anvh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae25f2be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f2d64f6-aeba-4149-f33f-5b1844f5f923"
      },
      "source": [
        "%pip install pydantic-ai"
      ],
      "id": "ae25f2be",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic-ai\n",
            "  Downloading pydantic_ai-1.0.15-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydantic-ai-slim==1.0.15 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading pydantic_ai_slim-1.0.15-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting genai-prices>=0.0.28 (from pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading genai_prices-0.0.29-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting griffe>=1.3.2 (from pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.28.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.37.0)\n",
            "Collecting pydantic-graph==1.0.15 (from pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading pydantic_graph-1.0.15-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.11.9)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.4.2)\n",
            "Collecting ag-ui-protocol>=0.1.8 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading ag_ui_protocol-0.1.9-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: starlette>=0.45.3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.48.0)\n",
            "Collecting anthropic>=0.69.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading anthropic-0.69.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting boto3>=1.39.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading boto3-1.40.45-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting argcomplete>=3.5.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: prompt-toolkit>=3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (3.0.52)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.11.0)\n",
            "Requirement already satisfied: rich>=13 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (13.9.4)\n",
            "Collecting cohere>=5.18.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading cohere-5.18.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pydantic-evals==1.0.15 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading pydantic_evals-1.0.15-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: google-genai>=1.31.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.39.1)\n",
            "Collecting groq>=0.25.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading groq-0.32.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.35.3)\n",
            "Collecting logfire>=3.14.1 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading logfire-4.11.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: mcp>=1.12.3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.15.0)\n",
            "Collecting mistralai>=1.9.10 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: openai>=1.107.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.109.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (8.5.0)\n",
            "Collecting temporalio==1.18.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading temporalio-1.18.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth>=2.36.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.38.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.32.4)\n",
            "Requirement already satisfied: anyio>=0 in /usr/local/lib/python3.12/dist-packages (from pydantic-evals==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (4.11.0)\n",
            "Collecting logfire-api>=3.14.1 (from pydantic-evals==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading logfire_api-4.11.0-py3-none-any.whl.metadata (972 bytes)\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from pydantic-evals==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (6.0.3)\n",
            "Collecting nexus-rpc==1.1.0 (from temporalio==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading nexus_rpc-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=3.20 in /usr/local/lib/python3.12/dist-packages (from temporalio==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (5.29.5)\n",
            "Collecting types-protobuf>=3.20 (from temporalio==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading types_protobuf-6.32.1.20250918-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from temporalio==1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.69.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.69.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.17.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.69.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic>=0.69.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.3.1)\n",
            "Collecting botocore<1.41.0,>=1.40.45 (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading botocore-1.40.45-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.39.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading fastavro-1.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.33.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.22.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (4.9.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai>=1.31.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (15.0.1)\n",
            "Collecting colorama>=0.4 (from griffe>=1.3.2->pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.1.10)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (3.12.15)\n",
            "Collecting executing>=2.0.1 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0 in /usr/local/lib/python3.12/dist-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.37.0)\n",
            "Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk<1.38.0,>=1.35.0 in /usr/local/lib/python3.12/dist-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.37.0)\n",
            "Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading opentelemetry_instrumentation_httpx-0.58b0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.11.0)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (3.0.2)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.37.0)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading invoke-2.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.28.0->pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (8.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit>=3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.2.14)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10->pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.7.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.19.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==1.0.15->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (3.23.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.27.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.1.2)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<1.38.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.58b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.17.3)\n",
            "Collecting opentelemetry-util-http==0.58b0 (from opentelemetry-instrumentation-httpx>=0.42b0->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai)\n",
            "  Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.17.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp>=1.12.3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (8.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,vertexai]==1.0.15->pydantic-ai) (1.20.1)\n",
            "Downloading pydantic_ai-1.0.15-py3-none-any.whl (11 kB)\n",
            "Downloading pydantic_ai_slim-1.0.15-py3-none-any.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.9/344.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_evals-1.0.15-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_graph-1.0.15-py3-none-any.whl (27 kB)\n",
            "Downloading temporalio-1.18.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nexus_rpc-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading ag_ui_protocol-0.1.9-py3-none-any.whl (7.1 kB)\n",
            "Downloading anthropic-0.69.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.45-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.18.0-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.4/295.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading genai_prices-0.0.29-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.32.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading logfire-4.11.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.9/226.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.45-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
            "Downloading fastavro-1.12.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading invoke-2.2.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading logfire_api-4.11.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl (33 kB)\n",
            "Downloading opentelemetry_instrumentation_httpx-0.58b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl (7.7 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_protobuf-6.32.1.20250918-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, types-protobuf, opentelemetry-util-http, nexus-rpc, logfire-api, jmespath, invoke, httpx-sse, fastavro, executing, eval-type-backport, colorama, argcomplete, temporalio, griffe, botocore, s3transfer, pydantic-graph, mistralai, groq, genai-prices, anthropic, ag-ui-protocol, pydantic-ai-slim, opentelemetry-instrumentation, cohere, boto3, pydantic-evals, opentelemetry-instrumentation-httpx, logfire, pydantic-ai\n",
            "  Attempting uninstall: httpx-sse\n",
            "    Found existing installation: httpx-sse 0.4.1\n",
            "    Uninstalling httpx-sse-0.4.1:\n",
            "      Successfully uninstalled httpx-sse-0.4.1\n",
            "Successfully installed ag-ui-protocol-0.1.9 anthropic-0.69.0 argcomplete-3.6.2 boto3-1.40.45 botocore-1.40.45 cohere-5.18.0 colorama-0.4.6 eval-type-backport-0.2.2 executing-2.2.1 fastavro-1.12.0 genai-prices-0.0.29 griffe-1.14.0 groq-0.32.0 httpx-sse-0.4.0 invoke-2.2.0 jmespath-1.0.1 logfire-4.11.0 logfire-api-4.11.0 mistralai-1.9.11 nexus-rpc-1.1.0 opentelemetry-instrumentation-0.58b0 opentelemetry-instrumentation-httpx-0.58b0 opentelemetry-util-http-0.58b0 pydantic-ai-1.0.15 pydantic-ai-slim-1.0.15 pydantic-evals-1.0.15 pydantic-graph-1.0.15 s3transfer-0.14.0 temporalio-1.18.0 types-protobuf-6.32.1.20250918 types-requests-2.32.4.20250913\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "opentelemetry"
                ]
              },
              "id": "370d1d1d94674004888d4fa26f2fe6da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Agents"
      ],
      "metadata": {
        "id": "txQM-X9qcnrg"
      },
      "id": "txQM-X9qcnrg"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
        "\n",
        "\n",
        "def log_entry(agent, messages, source=\"user\"):\n",
        "    tools = []\n",
        "\n",
        "    for ts in agent.toolsets:\n",
        "        tools.extend(ts.tools.keys())\n",
        "\n",
        "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
        "\n",
        "    return {\n",
        "        \"agent_name\": agent.name,\n",
        "        \"system_prompt\": agent._instructions,\n",
        "        \"provider\": agent.model.system,\n",
        "        \"model\": agent.model.model_name,\n",
        "        \"tools\": tools,\n",
        "        \"messages\": dict_messages,\n",
        "        \"source\": source\n",
        "    }"
      ],
      "metadata": {
        "id": "DUosv6gucr50"
      },
      "id": "DUosv6gucr50",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTjm35WZ9sIr",
        "outputId": "dcd1bde6-f1dc-4595-db03-d01c2b71e970"
      },
      "id": "iTjm35WZ9sIr",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Log"
      ],
      "metadata": {
        "id": "jp8BEZBldlvZ"
      },
      "id": "jp8BEZBldlvZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import secrets\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "LOG_DIR = Path('/content/drive/MyDrive/logs')\n",
        "LOG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "def serializer(obj):\n",
        "    if isinstance(obj, datetime):\n",
        "        return obj.isoformat()\n",
        "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
        "\n",
        "\n",
        "def log_interaction_to_file(agent, messages, source='user'):\n",
        "    entry = log_entry(agent, messages, source)\n",
        "\n",
        "    ts = entry['messages'][-1]['timestamp']\n",
        "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    rand_hex = secrets.token_hex(3)\n",
        "\n",
        "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
        "    filepath = LOG_DIR / filename\n",
        "\n",
        "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
        "        json.dump(entry, f_out, indent=2, default=serializer)\n",
        "\n",
        "    return filepath"
      ],
      "metadata": {
        "id": "n4Vh_5fsdkep"
      },
      "id": "n4Vh_5fsdkep",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = input()\n",
        "result = await agent.run(user_prompt=question)\n",
        "print(result.output)\n",
        "log_interaction_to_file(agent, result.new_messages())"
      ],
      "metadata": {
        "id": "igjntjNvd7_M"
      },
      "id": "igjntjNvd7_M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What was Apple's revenue in Q4 2022\"\n",
        "\n"
      ],
      "metadata": {
        "id": "xUtugZv250X0"
      },
      "execution_count": 37,
      "outputs": [],
      "id": "xUtugZv250X0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding References"
      ],
      "metadata": {
        "id": "Yb63jO2aePKz"
      },
      "id": "Yb63jO2aePKz"
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant for a course.\n",
        "\n",
        "Use the search tool to find relevant information from the course materials before answering questions.\n",
        "\n",
        "If you can find specific information through search, use it to provide accurate answers.\n",
        "\n",
        "Always include references by citing the filename of the source material you used.\n",
        "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
        "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
        "\n",
        "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Create another version of agent, let's call it faq_agent_v2\n",
        "agent = Agent(\n",
        "    name=\"faq_agent_v2\",\n",
        "    instructions=system_prompt,\n",
        "    tools=[text_search],\n",
        "    model='gpt-4o-mini'\n",
        ")\n"
      ],
      "metadata": {
        "id": "5gfpPbo9eOD7"
      },
      "id": "5gfpPbo9eOD7",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM as a Judge"
      ],
      "metadata": {
        "id": "q-4gamcee-JD"
      },
      "id": "q-4gamcee-JD"
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_prompt = \"\"\"\n",
        "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
        "We also include the entire log (<LOG>) for analysis.\n",
        "\n",
        "For each item, check if the condition is met.\n",
        "\n",
        "Checklist:\n",
        "\n",
        "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
        "- instructions_avoid: The agent avoided doing things it was told not to do\n",
        "- answer_relevant: The response directly addresses the user's question\n",
        "- answer_clear: The answer is clear and correct\n",
        "- answer_citations: The response includes proper citations or sources when required\n",
        "- completeness: The response is complete and covers all key aspects of the request\n",
        "- tool_call_search: Is the search tool invoked?\n",
        "\n",
        "Output true/false for each check and provide a short explanation for your judgment.\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "laHceNCSe8Ah"
      },
      "id": "laHceNCSe8Ah",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM will produce output that matches this schema exactly.\n"
      ],
      "metadata": {
        "id": "ZvOhfGHQf8rm"
      },
      "id": "ZvOhfGHQf8rm"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class EvaluationCheck(BaseModel):\n",
        "    check_name: str\n",
        "    justification: str\n",
        "    check_pass: bool\n",
        "\n",
        "class EvaluationChecklist(BaseModel):\n",
        "    checklist: list[EvaluationCheck]\n",
        "    summary: str"
      ],
      "metadata": {
        "id": "ZTjKBqHNf5uf"
      },
      "id": "ZTjKBqHNf5uf",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Pydantic AI in order to make the output follow the specified class, we use the parameter output_type:\n"
      ],
      "metadata": {
        "id": "kQEjARkSgOtR"
      },
      "id": "kQEjARkSgOtR"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_agent = Agent(\n",
        "    name='eval_agent',\n",
        "    model='gpt-5-nano',\n",
        "    instructions=evaluation_prompt,\n",
        "    output_type=EvaluationChecklist\n",
        ")\n"
      ],
      "metadata": {
        "id": "PISHs4tCgMRH"
      },
      "id": "PISHs4tCgMRH",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to run the agent, it needs input. We'll start with a template:\n"
      ],
      "metadata": {
        "id": "5wYgu6IAge1C"
      },
      "id": "5wYgu6IAge1C"
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt_format = \"\"\"\n",
        "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
        "<QUESTION>{question}</QUESTION>\n",
        "<ANSWER>{answer}</ANSWER>\n",
        "<LOG>{log}</LOG>\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "R_O_LZc4gbiG"
      },
      "id": "R_O_LZc4gbiG",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_log_file(log_file):\n",
        "    with open(log_file, 'r') as f_in:\n",
        "        log_data = json.load(f_in)\n",
        "        log_data['log_file'] = log_file\n",
        "        return log_data\n"
      ],
      "metadata": {
        "id": "OH0c17BIhnGw"
      },
      "id": "OH0c17BIhnGw",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_record = load_log_file('/content/drive/MyDrive/logs/faq_agent_v2_20250926_072928_467470.json')\n",
        "\n",
        "instructions = log_record['system_prompt']\n",
        "question = log_record['messages'][0]['parts'][0]['content']\n",
        "answer = log_record['messages'][-1]['parts'][0]['content']\n",
        "log = json.dumps(log_record['messages'])\n",
        "\n",
        "user_prompt = user_prompt_format.format(\n",
        "    instructions=instructions,\n",
        "    question=question,\n",
        "    answer=answer,\n",
        "    log=log\n",
        ")\n"
      ],
      "metadata": {
        "id": "A4Aa1O3wjCql"
      },
      "id": "A4Aa1O3wjCql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The user input is ready and we can test it!"
      ],
      "metadata": {
        "id": "pUHJ_CSnjIu-"
      },
      "id": "pUHJ_CSnjIu-"
    },
    {
      "cell_type": "code",
      "source": [
        "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
        "\n",
        "checklist = result.output\n",
        "print(checklist.summary)\n",
        "\n",
        "for check in checklist.checklist:\n",
        "    print(check)"
      ],
      "metadata": {
        "id": "jv6fJh8_jGnG"
      },
      "id": "jv6fJh8_jGnG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we're putting the entire conversation log into the prompt, which is not really necessary. We can reduce it to make it less verbose.\n"
      ],
      "metadata": {
        "id": "mEyS1U09jjZx"
      },
      "id": "mEyS1U09jjZx"
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_log_messages(messages):\n",
        "    log_simplified = []\n",
        "\n",
        "    for m in messages:\n",
        "        parts = []\n",
        "\n",
        "        for original_part in m['parts']:\n",
        "            part = original_part.copy()\n",
        "            kind = part['part_kind']\n",
        "\n",
        "            if kind == 'user-prompt':\n",
        "                del part['timestamp']\n",
        "            if kind == 'tool-call':\n",
        "                del part['tool_call_id']\n",
        "            if kind == 'tool-return':\n",
        "                del part['tool_call_id']\n",
        "                del part['metadata']\n",
        "                del part['timestamp']\n",
        "                # Replace actual search results with placeholder to save tokens\n",
        "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
        "            if kind == 'text':\n",
        "                del part['id']\n",
        "\n",
        "            parts.append(part)\n",
        "\n",
        "        message = {\n",
        "            'kind': m['kind'],\n",
        "            'parts': parts\n",
        "        }\n",
        "\n",
        "        log_simplified.append(message)\n",
        "    return log_simplified\n"
      ],
      "metadata": {
        "id": "-QDzlvFRjgia"
      },
      "id": "-QDzlvFRjgia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def evaluate_log_record(eval_agent, log_record):\n",
        "    messages = log_record['messages']\n",
        "\n",
        "    instructions = log_record['system_prompt']\n",
        "    question = messages[0]['parts'][0]['content']\n",
        "    answer = messages[-1]['parts'][0]['content']\n",
        "\n",
        "    log_simplified = simplify_log_messages(messages)\n",
        "    log = json.dumps(log_simplified)\n",
        "\n",
        "    user_prompt = user_prompt_format.format(\n",
        "        instructions=instructions,\n",
        "        question=question,\n",
        "        answer=answer,\n",
        "        log=log\n",
        "    )\n",
        "\n",
        "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
        "    return result.output\n",
        "\n",
        "\n",
        "log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
        "eval1 = await evaluate_log_record(eval_agent, log_record)"
      ],
      "metadata": {
        "id": "KrZ1zSrWjqZo"
      },
      "id": "KrZ1zSrWjqZo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generation\n",
        "We can ask AI to help. What if we used it for generating more questions? Let's do that.\n",
        "We can sample some records from our database. Then for each record, ask an LLM to generate a question based on the record. We use this question as input to our agent and log the answers.\n",
        "Let’s start by defining the question generator:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T2O27TjtjwiZ"
      },
      "id": "T2O27TjtjwiZ"
    },
    {
      "cell_type": "code",
      "source": [
        "question_generation_prompt = \"\"\"\n",
        "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
        "\n",
        "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
        "\n",
        "The questions should:\n",
        "\n",
        "- Be natural and varied in style\n",
        "- Range from simple to complex\n",
        "- Include both specific technical questions and general course questions\n",
        "\n",
        "Generate one question for each record.\n",
        "\"\"\".strip()\n",
        "\n",
        "class QuestionsList(BaseModel):\n",
        "    questions: list[str]\n",
        "\n",
        "question_generator = Agent(\n",
        "    name=\"question_generator\",\n",
        "    instructions=question_generation_prompt,\n",
        "    model='gpt-4o-mini',\n",
        "    output_type=QuestionsList\n",
        ")\n"
      ],
      "metadata": {
        "id": "2Tra8ZgNnUAn"
      },
      "id": "2Tra8ZgNnUAn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we simply iterate over each of the question, ask our agent and log the results:\n"
      ],
      "metadata": {
        "id": "wSsVPVvuoEXa"
      },
      "id": "wSsVPVvuoEXa"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sample = random.sample(fin_faq, 10)\n",
        "prompt_docs = [d['content'] for d in sample]\n",
        "prompt = json.dumps(prompt_docs)\n",
        "\n",
        "result = await question_generator.run(prompt)\n",
        "questions = result.output.questions"
      ],
      "metadata": {
        "id": "HETLo8e5nyol"
      },
      "id": "HETLo8e5nyol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, collect all the AI-generated logs for the v2 agent:"
      ],
      "metadata": {
        "id": "w7vtMEZNoSP_"
      },
      "id": "w7vtMEZNoSP_"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_set = []\n",
        "\n",
        "for log_file in LOG_DIR.glob('*.json'):\n",
        "    if 'faq_agent_v2' not in log_file.name:\n",
        "        continue\n",
        "\n",
        "    log_record = load_log_file(log_file)\n",
        "    if log_record['source'] != 'ai-generated':\n",
        "        continue\n",
        "\n",
        "    eval_set.append(log_record)"
      ],
      "metadata": {
        "id": "H2B5n6cZoPs9"
      },
      "id": "H2B5n6cZoPs9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And evaluate them:"
      ],
      "metadata": {
        "id": "a8DbUbPWoXoy"
      },
      "id": "a8DbUbPWoXoy"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = []\n",
        "\n",
        "for log_record in tqdm(eval_set):\n",
        "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
        "    eval_results.append((log_record, eval_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ffd87085bbd440db9e447f74be90599f",
            "0329cfd3d6024b75a5719722e5fa0ffd",
            "5bb2330d868a416ebd244094c14b94c3",
            "33ab75cb80624ca7990461ce3b705984",
            "f7dc15ff2fd24e23a7f9b878df6819cb",
            "bc065848fb5640888090a10ddd07d039",
            "9c89699de134478f992be8a6c7c33ac4",
            "00d59f07d42a46bbbbb7b6578479e806",
            "14f9f43abb344eadaf04b5e29e3dce09",
            "2eb41c7a6ddc413f867a519492d5a358",
            "6d85e2e1ec78438a8d51572b308c1847"
          ]
        },
        "id": "1VHPv60foVQr",
        "outputId": "c3e53c48-d76d-49c8-e8fb-3a1c92fed211"
      },
      "id": "1VHPv60foVQr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffd87085bbd440db9e447f74be90599f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code:\n",
        "Loops through each AI-generated log\n",
        "Runs our evaluation agent on it\n",
        "Stores both the original log and evaluation result\n"
      ],
      "metadata": {
        "id": "0H8jze5kocoA"
      },
      "id": "0H8jze5kocoA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are collected, but we need to display them and also calculate some statistics. The best tool for doing this is Pandas. We already should have it because minsearch depends on it.\n"
      ],
      "metadata": {
        "id": "XKiXIURqolJV"
      },
      "id": "XKiXIURqolJV"
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for log_record, eval_result in eval_results:\n",
        "    messages = log_record['messages']\n",
        "\n",
        "    row = {\n",
        "        'file': log_record['log_file'].name,\n",
        "        'question': messages[0]['parts'][0]['content'],\n",
        "        'answer': messages[-1]['parts'][0]['content'],\n",
        "    }\n",
        "\n",
        "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
        "    row.update(checks)\n",
        "\n",
        "    rows.append(row)"
      ],
      "metadata": {
        "id": "aJIYh_lsopMu"
      },
      "id": "aJIYh_lsopMu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code:\n",
        "Extracts key information from each log (file, question, answer)\n",
        "Converts the evaluation checks into a dictionary format\n"
      ],
      "metadata": {
        "id": "exWpJHZ_ouxh"
      },
      "id": "exWpJHZ_ouxh"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_evals = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "hZ8Ul3Ctoz9O"
      },
      "id": "hZ8Ul3Ctoz9O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_evals.mean(numeric_only=True)"
      ],
      "metadata": {
        "id": "qN-IRfg-o7hm"
      },
      "id": "qN-IRfg-o7hm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "instructions_follow    0.3\n",
        "instructions_avoid     1.0\n",
        "answer_relevant        1.0\n",
        "answer_clear           1.0\n",
        "answer_citations       0.3\n",
        "completeness           0.7\n",
        "tool_call_search       1.0\n"
      ],
      "metadata": {
        "id": "-ht5lAeHo-l_"
      },
      "id": "-ht5lAeHo-l_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tells us:\n",
        "Only 30% of responses follow instructions completely\n",
        "All responses avoid forbidden actions (good!)\n",
        "All responses are relevant and clear (great!)\n",
        "Only 30% include proper citations (needs improvement)\n",
        "70% of responses are complete\n",
        "All responses use the search tool (as expected)\n"
      ],
      "metadata": {
        "id": "aRZAp7Q1o_1L"
      },
      "id": "aRZAp7Q1o_1L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating functions and tools\n"
      ],
      "metadata": {
        "id": "A_iAtl9oqhzu"
      },
      "id": "A_iAtl9oqhzu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we can implement hitrate and MRR calculation in Python:"
      ],
      "metadata": {
        "id": "sIXCUkKZ6Tac"
      },
      "id": "sIXCUkKZ6Tac"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_search_quality(search_function, test_queries):\n",
        "    results = []\n",
        "\n",
        "    for query, expected_docs in test_queries:\n",
        "        search_results = search_function(query, num_results=5)\n",
        "\n",
        "        # Calculate hit rate\n",
        "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
        "\n",
        "        # Calculate MRR\n",
        "        for i, doc in enumerate(search_results):\n",
        "            if doc['filename'] in expected_docs:\n",
        "                mrr = 1 / (i + 1)\n",
        "                break\n",
        "        else:\n",
        "            mrr = 0\n",
        "\n",
        "        results.append({\n",
        "            'query': query,\n",
        "            'hit': relevant_found,\n",
        "            'mrr': mrr\n",
        "        })\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Xqw19DuNq_9a"
      },
      "id": "Xqw19DuNq_9a",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ffd87085bbd440db9e447f74be90599f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0329cfd3d6024b75a5719722e5fa0ffd",
              "IPY_MODEL_5bb2330d868a416ebd244094c14b94c3",
              "IPY_MODEL_33ab75cb80624ca7990461ce3b705984"
            ],
            "layout": "IPY_MODEL_f7dc15ff2fd24e23a7f9b878df6819cb"
          }
        },
        "0329cfd3d6024b75a5719722e5fa0ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc065848fb5640888090a10ddd07d039",
            "placeholder": "​",
            "style": "IPY_MODEL_9c89699de134478f992be8a6c7c33ac4",
            "value": ""
          }
        },
        "5bb2330d868a416ebd244094c14b94c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00d59f07d42a46bbbbb7b6578479e806",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14f9f43abb344eadaf04b5e29e3dce09",
            "value": 0
          }
        },
        "33ab75cb80624ca7990461ce3b705984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eb41c7a6ddc413f867a519492d5a358",
            "placeholder": "​",
            "style": "IPY_MODEL_6d85e2e1ec78438a8d51572b308c1847",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "f7dc15ff2fd24e23a7f9b878df6819cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc065848fb5640888090a10ddd07d039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c89699de134478f992be8a6c7c33ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00d59f07d42a46bbbbb7b6578479e806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "14f9f43abb344eadaf04b5e29e3dce09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2eb41c7a6ddc413f867a519492d5a358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d85e2e1ec78438a8d51572b308c1847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94f31ef13c224820951c1ea56985b42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dfb81d901f041d0aec4715ad46abad1",
              "IPY_MODEL_b5be2cb5660145de98e45e5c3288c07b",
              "IPY_MODEL_9df1e2264aae49de86ff665e28c8a5a8"
            ],
            "layout": "IPY_MODEL_78a73ecb7fe54a7e8d37647e2a5ca5f4"
          }
        },
        "5dfb81d901f041d0aec4715ad46abad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f7fc392e4f44d7b43ed1cbb52e9665",
            "placeholder": "​",
            "style": "IPY_MODEL_8374e65734044cd289d0a463fc7d1b82",
            "value": "100%"
          }
        },
        "b5be2cb5660145de98e45e5c3288c07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7460c485045146dab83a450caf7ccd55",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d749ad8d30d4112b70684a0dee8c273",
            "value": 5
          }
        },
        "9df1e2264aae49de86ff665e28c8a5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e45c83e78694e9782006150d8639df6",
            "placeholder": "​",
            "style": "IPY_MODEL_5d05715b10654fc087cf3131d44792b1",
            "value": " 5/5 [00:00&lt;00:00, 12.68it/s]"
          }
        },
        "78a73ecb7fe54a7e8d37647e2a5ca5f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35f7fc392e4f44d7b43ed1cbb52e9665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8374e65734044cd289d0a463fc7d1b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7460c485045146dab83a450caf7ccd55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d749ad8d30d4112b70684a0dee8c273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e45c83e78694e9782006150d8639df6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d05715b10654fc087cf3131d44792b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}